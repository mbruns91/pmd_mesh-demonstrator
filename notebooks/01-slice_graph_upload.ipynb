{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade9d5d2-9cd5-4249-af62-db268474732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load InteactiveShell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112522f9-3018-47b5-9c19-0d36e3e2a652",
   "metadata": {},
   "source": [
    "# Slice the dataset and upload via the mesh\n",
    "\n",
    "In this notebook, we slice the original dataset (stored in `../pmdco2_tto_example.ttl`) and distribute the slices to 3 `ontodocker` instances on the mesh. We will slice it into 3 pieces corresponding to the orientation in which the respective specimen was cut from the steel-sheet relative to the rolling direction. We then serialize the generated graphs as turtle-files and do the upload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895d9a66-c234-4d8c-ae39-2d120beab437",
   "metadata": {},
   "source": [
    "## Create three subgraphs\n",
    "These orientations are *parallel*, *perpendicular* and *diagonal*. Each of the resulting graphs/ datasets should contain all \"general\" information of the original dataset, i.e. information like general metadata, which cannot be assigned to a specific orientation.\n",
    "\n",
    "To do that, we have to\n",
    "- collect all processes with a rolling direction\n",
    "- collect all other triples belonging to the resp. process\n",
    "- collect all triples which are not related to a rolling diretion\n",
    "\n",
    "The resulting graphs then are finally serialized as turtle-files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de93f02f-d63d-452a-9ac5-676c30df984b",
   "metadata": {},
   "source": [
    "First, we import the required objects from `rdflib` and define namespaces which are used:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c46209-9df2-4894-bbf1-050e12482986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "\n",
    "ns_pmdco = Namespace(\"https://w3id.org/pmd/co/\")\n",
    "ns_tte = Namespace(\"<https://w3id.org/pmd/ao/tte/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b1ad7-52ae-4e3b-9294-f3381d2446a1",
   "metadata": {},
   "source": [
    "Instantiate the `Graph()`-object for the original (full) graph/dataset and pars the data into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6afd9529-7d12-43fd-9a51-8c61e0f1802c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N1b398964f0ae4aabbcfd235dedc748d2 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph()\n",
    "g.parse(\"../pmdco2_tto_example.ttl\", format=\"turtle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146d96c-ace6-4d23-9de8-9e6478c9edf5",
   "metadata": {},
   "source": [
    "Now, we collect all **processes** accossiated with a rolling direction. Using `set()` prevents the occurance of doubled information (dublicates). We do this by iterating over all triples from the full graph `g` and searching all triples with predicate \"pmdco.value\" and the string \"*_rollingDirection\" in their subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efdc4eb6-97bd-4415-a1ab-167377df11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processes_parallel = set()\n",
    "processes_perpendicular = set()\n",
    "processes_diagonal = set()\n",
    "\n",
    "for rolling_dir in g.subjects(predicate=ns_pmdco.value, object=None):\n",
    "    if \"_rollingDirection\" in str(rolling_dir):\n",
    "        for s, p, o in g.triples((rolling_dir, ns_pmdco.value, None)):\n",
    "            value = str(o).strip().lower()\n",
    "            for proc in g.subjects(predicate=ns_pmdco.characteristic, object=rolling_dir):\n",
    "                if value == \"in rolling direction\":\n",
    "                    processes_parallel.add(proc)\n",
    "                elif value == \"perpendicular to rolling direction\":\n",
    "                    processes_perpendicular.add(proc)\n",
    "                elif value == \"diagonal to rolling direction\":\n",
    "                    processes_diagonal.add(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d97b3-aa26-4d79-bb11-b91550b47c98",
   "metadata": {},
   "source": [
    "Next, we collect al other triples, which are acossiated with the processes from above. For this, we define a function iterating over all triples as long as the triples IRI is from the TTE namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57d85623-f136-4b33-9aaf-bd2a539d19e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_tripels(process, graph, collected=None):\n",
    "    if collected is None:\n",
    "        collected = set()\n",
    "    for t in graph.triples((process, None, None)):\n",
    "        if t not in collected:\n",
    "            collected.add(t)\n",
    "            # collect recursively, if object (t[2]) is \"URIRef\" (IRI) in the same namespace (TTE)\n",
    "            if isinstance(t[2], URIRef) and str(t[2]).startswith(str(ns_tte)):\n",
    "                collect_tripels(t[2], graph, collected)\n",
    "    return collected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa637c1-e17f-4171-bc1a-93d82fcb09ce",
   "metadata": {},
   "source": [
    "Now we actualy collect the triples by iterating over all triples and creating the unions of triples belonging to a process (which, in turn, is accossiated with an orientation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c340a29e-ee76-4450-897c-97afaebc8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripels_parallel = set()\n",
    "tripels_perpendicular = set()\n",
    "tripels_diagonal = set()\n",
    "\n",
    "for proc in processes_parallel:\n",
    "    tripels_parallel |= collect_tripels(proc, g) # (a|=b) == (a = a|b) == (a.update(b)) == (a = a.union(b)); in-place Union/Vereinigung\n",
    "for proc in processes_perpendicular:\n",
    "    tripels_perpendicular |= collect_tripels(proc, g)\n",
    "for proc in processes_diagonal:\n",
    "    tripels_diagonal |= collect_tripels(proc, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9dbc4-1d2d-4dc7-9786-8482e3226a1d",
   "metadata": {},
   "source": [
    "Also, we need to collect general information which is not related to a process at all. We do this, by first creating the graph which *only* contains information related to a process and substracting this from the overall full graph `g`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d959126-8b6b-4212-a58a-d5cb533eaa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_tripels = tripels_parallel | tripels_perpendicular | tripels_diagonal # (c = a | b) == (c = a.union(b)); Union/Vereinigung\n",
    "general_tripels = set(g) - rolling_tripels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c9459-ce52-4ad1-a1f3-efb4a2def3cf",
   "metadata": {},
   "source": [
    "Finally, we create the graphs from the union of the resp. (set of process-realted) triples and the general triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32fe4b04-22bd-49aa-8968-dc45b33554b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "g_parallel = Graph()\n",
    "g_perpendicular = Graph()\n",
    "g_diagonal = Graph()\n",
    "for t in tripels_parallel | general_tripels:\n",
    "    g_parallel.add(t)\n",
    "for t in tripels_perpendicular | general_tripels:\n",
    "    g_perpendicular.add(t)\n",
    "for t in tripels_diagonal | general_tripels:\n",
    "    g_diagonal.add(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0180e3-a3b4-4c48-a9fc-cf3d96b51353",
   "metadata": {},
   "source": [
    "Define handles for the datasets and the related files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acf96e2b-a900-4cd4-bbb0-d2d8560872b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetname_parallel = \"pmdco2_tto_example_parallel\"\n",
    "filename_parallel = \"../\" + datasetname_parallel + \".ttl\"\n",
    "\n",
    "datasetname_perpendicular = \"pmdco2_tto_example_perpendicular\"\n",
    "filename_perpendicular = \"../\" + datasetname_perpendicular + \".ttl\"\n",
    "\n",
    "datasetname_diagonal = \"pmdco2_tto_example_diagonal\"\n",
    "filename_diagonal = \"../\" + datasetname_diagonal + \".ttl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e6140b-069c-4484-88c9-1f04a2a82034",
   "metadata": {},
   "source": [
    "Serialize the graphs to turtle-files (`.ttl`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68f3d007-0c3c-44a2-bb5c-2ad1abf615ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N85fe88645da14c69b1722b7d5df147a5 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N849e719f1b054bc49ccc46a8cbb0ad39 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N44213d5354b740d1a3e8f0db04e08b1e (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_parallel.serialize(filename_parallel, format=\"turtle\")\n",
    "g_perpendicular.serialize(filename_perpendicular, format=\"turtle\")\n",
    "g_diagonal.serialize(filename_diagonal, format=\"turtle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b538f-36b0-4814-87fe-864bace44c19",
   "metadata": {},
   "source": [
    "## Upload the three slices via the mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd02c6d-65b4-4200-914c-66a9bd9ea16c",
   "metadata": {},
   "source": [
    "Before any usage of the mesh, we have to point `requests` to the right certificate bundle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42b58ce0-5072-42c6-a237-45c9abb97242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = \"/etc/ssl/certs/ca-certificates.crt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812745f2-c547-4011-954b-82864e1f5e13",
   "metadata": {},
   "source": [
    "Load the mesh-participant information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5553754f-0df6-4c60-9cd2-b5eeac0ebe75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import helpers\n",
    "\n",
    "import json\n",
    "\n",
    "with open('../secrets/participant_registry.json') as f:\n",
    "    partners = json.load(f, object_hook=lambda d: helpers.RecursiveNamespace(**d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f20b1-8b85-41c5-b934-5092b9a25a08",
   "metadata": {},
   "source": [
    "Do the upload via http requests to the ontodocker api (see also `00-technical_checks.ipynb`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "098d8e55-f780-4504-900a-1b99fe4a7c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating & filling dataset at \"https://ontodocker.iwm.pmd.internal\":\n",
      "--> \"Name already registered /pmdco2_tto_example_parallel\\n\"\n",
      "Upload \"../pmdco2_tto_example_parallel.ttl\" to dataset \"pmdco2_tto_example_parallel\" at \"https://ontodocker.iwm.pmd.internal\"\n",
      "--> \"Upload succeeded { \\n  \\\"count\\\" : 2476 ,\\n  \\\"tripleCount\\\" : 2476 ,\\n  \\\"quadCount\\\" : 0\\n}\\n\"\n",
      "\n",
      "Creating & filling dataset at \"https://ontodocker.iwt.pmd.internal\":\n",
      "--> \"Name already registered /pmdco2_tto_example_perpendicular\\n\"\n",
      "Upload \"../pmdco2_tto_example_perpendicular.ttl\" to dataset \"pmdco2_tto_example_perpendicular\" at \"https://ontodocker.iwt.pmd.internal\"\n",
      "--> \"Upload succeeded { \\n  \\\"count\\\" : 2476 ,\\n  \\\"tripleCount\\\" : 2476 ,\\n  \\\"quadCount\\\" : 0\\n}\\n\"\n",
      "\n",
      "Creating & filling dataset at \"https://ontodocker.mpi-susmat.pmd.internal\":\n",
      "--> \"Name already registered /pmdco2_tto_example_diagonal\\n\"\n",
      "Upload \"../pmdco2_tto_example_diagonal.ttl\" to dataset \"pmdco2_tto_example_diagonal\" at \"https://ontodocker.mpi-susmat.pmd.internal\"\n",
      "--> \"Upload succeeded { \\n  \\\"count\\\" : 2384 ,\\n  \\\"tripleCount\\\" : 2384 ,\\n  \\\"quadCount\\\" : 0\\n}\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "datasetname_list = [datasetname_parallel, datasetname_perpendicular, datasetname_diagonal]\n",
    "filename_list = [filename_parallel, filename_perpendicular, filename_diagonal]\n",
    "\n",
    "for (key, datasetname, filename) in zip(partners.__dict__, datasetname_list, filename_list):\n",
    "    address = getattr(partners, key).ontodocker.address\n",
    "    token = getattr(partners, key).ontodocker.token\n",
    "    \n",
    "    endpoint = f'{address}/api/v1/jena/{datasetname}'\n",
    "    headers = {\"Authorization\": f'Bearer {token}'}\n",
    "\n",
    "    # create dataset\n",
    "    print(f'Creating & filling dataset at \"{address}\":')\n",
    "    print(\"--> \"+ requests.put(endpoint, headers=headers).content.decode())\n",
    "\n",
    "    # uplaod file\n",
    "    print(f'Upload \"{filename}\" to dataset \"{datasetname}\" at \"{address}\"')\n",
    "    print(\"--> \" + requests.post(endpoint, headers=headers, files={'file': open(filename, 'rb')}).content.decode())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a2c5f-6695-4cb6-b47a-eec534cf1baa",
   "metadata": {},
   "source": [
    "**Deletion** (uncomment if you want to delete the datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3fd7f2b-7a58-40d1-abd9-5f05c56de32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for (key, datasetname) in zip(partners.__dict__, datasetname_list):\n",
    "#    address = getattr(partners, key).ontodocker.address\n",
    "#    token = getattr(partners, key).ontodocker.token\n",
    "#    \n",
    "#    endpoint = f'{address}/api/v1/jena/{datasetname}'\n",
    "#    headers = {\"Authorization\": f'Bearer {token}'}\n",
    "#    \n",
    "#    print(f'Deleting dataset \"{datasetname}\" at \"{address}\":')\n",
    "#    print(f'Endpoint was \"{endpoint}\"')\n",
    "#    print(requests.delete(endpoint, headers=headers).content.decode()+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beffad63-0c29-460e-8e56-966136f1d3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
